{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(object):\n",
    "    \"\"\"Class to extract and process vocabularies for mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {\n",
    "            idx: token for token, idx in self._token_to_idx.items()\n",
    "        }\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "        \n",
    "        # add begin and end sequence token\n",
    "        self._begin_of_seq_token = \"<BEGIN-OF-SEQUENCE>\"\n",
    "        self._end_of_seq_token = \"<END-OF-SEQUENCE>\"\n",
    "        \n",
    "        self.begin_seq_index = self.add_token(self._begin_of_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_of_seq_token)\n",
    "\n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializeable(self):\n",
    "        \"\"\"return a serializeable dictionary\"\"\"\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx,\n",
    "            'mask_token': self._mask_token,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token': self._unk_token\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializeable(cls, contents):\n",
    "        \"\"\"create vocabulary object from serialize dictionary\"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add a token and return it's index\"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"get the index of a token \n",
    "        if not exist returns the unk_index\"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTVectorizer(object):\n",
    "    def __init__(self, source_vocab, target_vocab,\n",
    "                max_source_length, max_target_length):\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, lang_df):\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        \n",
    "        max_source_length, max_target_length = 0, 0\n",
    "        \n",
    "        for rows in lang_df.iterrows():\n",
    "            # source\n",
    "            source_token = rows[\"source_languange\"].split(\" \")\n",
    "            if len(source_token) > max_source_length:\n",
    "                max_source_length = len(source_token)\n",
    "            for token in source_token:\n",
    "                source_vocab.add_token(token)\n",
    "            \n",
    "            # target\n",
    "            target_token = rows[\"target_language\"].split(\" \")\n",
    "            if len(target_token) > max_target_length:\n",
    "                max_target_length = len(target_token)\n",
    "            for token in target_token:\n",
    "                target_vocab.add_token(token)\n",
    "                \n",
    "        return cls(source_vocab, target_vocab,\n",
    "                  max_source_length, max_target_length)\n",
    "    \n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "        return vector\n",
    "    \n",
    "    def _get_source_indices(self, source_text):\n",
    "        \"\"\"\n",
    "        Source indices adding begin_seq_index and\n",
    "        end_seq_index\n",
    "        \"\"\"\n",
    "        indices = [self.source_vocab.begin_seq_index]\n",
    "        indices.extend(self.source_vocab.lookup_token(token) for token in\n",
    "                       source_text.split(\" \"))\n",
    "        indices.append(self.source_vocab.end_seq_index)\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def _get_target_indices(self, target_text):\n",
    "        indices = [self.target_vocab.lookup_token(token)\n",
    "                   for token in target_text.split(\" \")]\n",
    "        \n",
    "        x_indices = [self.target_vocab.begin_seq_index] + indices\n",
    "        y_indices = indices + [self.target_vocab.end_seq_index]\n",
    "        \n",
    "        return x_indices, y_indices\n",
    "    \n",
    "    def vectorize(self, source_text, target_text, use_dataset_max_length=True):\n",
    "        source_length = -1\n",
    "        target_length = -1\n",
    "        \n",
    "        if use_dataset_max_length:\n",
    "            source_length = self.max_source_length + 2\n",
    "            target_length = self.max_target_length + 1\n",
    "        \n",
    "        source_indices = self._get_source_indices(source_text)\n",
    "        source_vector = self._vectorize(source_indices,\n",
    "                                       source_length,\n",
    "                                       mask_index= self.source_vocab.mask_index)\n",
    "        \n",
    "        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
    "        \n",
    "        target_x_vector = self._vectorize(target_x_indices,\n",
    "                                         target_length,\n",
    "                                         self.target_vocab.mask_index)\n",
    "        target_y_vector = self._vectorize(target_y_indices,\n",
    "                                         target_length,\n",
    "                                         self.target_vocab.mask_index)\n",
    "        return {\"source_vector\": source_vector,\n",
    "                \"target_x_vector\": target_x_vector,\n",
    "                \"target_y_vector\": target_y_vector,\n",
    "                \"source_length\": len(source_indices)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, text_df, vectorizer):\n",
    "        self.text_df = text_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self._max_seq_length = max(map(len, self.text_df.surname)) + 2\n",
    "        \n",
    "        self.train_df = self.text_df[self.text_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.text_df[self.text_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.text_df[self.text_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, text_csv):\n",
    "        \"\"\"Load dataset from csv and returns the dataset object\n",
    "        and vectorizer\"\"\"\n",
    "        text_df = pd.read_csv(text_csv)\n",
    "        train_text_df = surname_df[text_df.split == 'train']\n",
    "        return cls(text_df,\n",
    "                   NMTVectorizer.from_dataframe(train_text_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Get vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        \"\"\"Set the split from data\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dict of the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        data_dict = self._vectorizer.vectorize(self.test_df['source_language'],\n",
    "                                                                    self.test_df['target_language'])\n",
    "        \n",
    "        return {\n",
    "            'x_source': data_dict[\"source_vector\"],\n",
    "            'x_target': data_dict[\"target_x_vector\"],\n",
    "            'y_target': data_dict[\"target_y_vector\"],\n",
    "            'x_source_length': data_dict[\"source_length\"]\n",
    "        }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given the batch size return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "\n",
    "def generate_nmt_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Batch Generator\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last= drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['x_source_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        \n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class NMTEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
    "        super(NMTEncoder, self).__init__()\n",
    "        \n",
    "        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
    "        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    def forward(self, x_source, x_lengths):\n",
    "        x_embedded = self.source_embedding(x_source)\n",
    "        x_lengths = x_lengths.detach().cpu().numpy()\n",
    "        x_packed = pack_padded_sequence(x_embedded, x_lengths, batch_first=True)\n",
    "        \n",
    "        x_birnn_out, x_birnn_h = self.birnn(x_packed)\n",
    "        x_birnn_h = x_birnn_h.permute(1,0,2)\n",
    "        \n",
    "        x_birnn_h = x_birnn_h.contigous().view(x_birnn_h.size(0), -1)\n",
    "        \n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
    "        return x_unpacked, x_birnn_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_attention(encoder_state_vectors, query_vector):\n",
    "    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\n",
    "    \n",
    "    vector_scores = \\\n",
    "        torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), dim=2)\n",
    "    \n",
    "    vector_probabilities = torch.softmax(vector_scores, dim=1)\n",
    "    \n",
    "    weighted_vectors = \\\n",
    "        encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\n",
    "    \n",
    "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
    "    return context_vectors, vector_probabilities\n",
    "\n",
    "def terse_attention(encoder_state_vectors, query_vector):\n",
    "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n",
    "    \n",
    "    vector_probabilities = torch.softmax(encoder_state_vectors, dim=-1)\n",
    "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),\n",
    "                                  vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
    "    \n",
    "    return context_vectors, vector_probabilities\n",
    "\n",
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        \n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                            embedding_dim=embedding_size,\n",
    "                                            padding_idx=0)\n",
    "        \n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        \n",
    "        self.classifier = nn.Linear(rnn_hidden_size*2, num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "        \n",
    "    def _init_indices(self, batch_size):\n",
    "        \"\"\"returns the BOS index vector\"\"\"\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "            \n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        \"\"\"returns a zeros vector for initializing the context\"\"\"\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
    "    \n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n",
    "        target_sequence = target_sequence.permute(1,0)\n",
    "        \n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "        \n",
    "        batch_size = encoder_state.size(0)\n",
    "        \n",
    "        # initialize context vector\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "        \n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        output_sequence_size = target_sequence.size(0)\n",
    "        for i in range(output_sequence_size):\n",
    "            \n",
    "            # decoding the vectors\n",
    "            # 1. embed word and concat with previous context\n",
    "            y_input_vector = self.target_embedding(target_sequence)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            \n",
    "            # 2. make a GRU step, getting a new hidden vector\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().data.numpy())\n",
    "            \n",
    "            # 3. use current vector to attend to encoder state\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state, h_t)\n",
    "            \n",
    "            # cache the attention probabilities for visualization\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            \n",
    "            # 4 use current hidden and context vectors\n",
    "            # to make a prediction for the next word\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(prediction_vector)\n",
    "            \n",
    "            # collect the prediction scores\n",
    "            output_vectors.append(score_for_y_t_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(self, source_vocab_size, source_embedding_size, target_vocab_size,\n",
    "                target_embedding_size, encoding_size, target_bos_index):\n",
    "        super(NMTModel, self).__init__()\n",
    "        \n",
    "        self.encoder = NMTEncoder(num_embeddings= source_vocab_size,\n",
    "                                 embedding_size=source_embedding_size,\n",
    "                                 rnn_hidden_size=encoding_size)\n",
    "        \n",
    "        decoding_size = encoding_size * 2\n",
    "        \n",
    "        self.decoder = NMTDecoder(num_embeddings= target_vocab_size,\n",
    "                                 embedding_size= target_embedding_size,\n",
    "                                 rnn_hidden_size= decoding_size,\n",
    "                                 bos_index= target_bos_index)\n",
    "        \n",
    "    def forward(self, x_source, x_source_lengths, target_sequence):\n",
    "        encoder_state, final_hidden_states = self.encoder(x_source,\n",
    "                                                         x_source_lengths)\n",
    "        decoded_states = self.decoder(encoder_state, final_hidden_states,\n",
    "                                     target_sequence)\n",
    "        \n",
    "        return decoded_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data information\n",
    "    frequency_cutoff = 25,\n",
    "    text_csv = '/content/drive/My Drive/Colab Notebooks/Data/surnames_with_splits.csv',\n",
    "    # Model HyperParameters\n",
    "    source_embedding_size=24, \n",
    "    target_embedding_size=24,\n",
    "    encoding_size=32,\n",
    "    # Training HyperParameters\n",
    "    batch_size = 128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    momentum=0.1,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    cuda=True,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {\n",
    "        'epoch_index':0,\n",
    "        'train_loss':[],\n",
    "        'train_acc':[],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': -1,\n",
    "        'test_acc': -1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "  args.cuda = True\n",
    "else:\n",
    "  args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Device available \", args.device)\n",
    "\n",
    "# dataset object\n",
    "dataset = NMTDataset.load_dataset_and_make_vectorizer(args.text_csv)\n",
    "\n",
    "# vectorizer\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# classifier\n",
    "model = NMTModel(source_vocab_size= len(vectorizer.source_vocab),\n",
    "                target_vocab_size= len(vectorizer.target_vocab),\n",
    "                source_embedding_size = args.source_embedding_size,\n",
    "                target_embedding_size= args.target_embedding_size,\n",
    "                encoding_size= args.encoding_size,\n",
    "                target_bos_index= vectorizer.target_vocab.begin_seq_index)\n",
    "model.to(args.device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_index = vectorizer.target_vocab.mask_index\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size)-1, \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size)-1, \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = model(batch_dict['x_source'], \n",
    "                           batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        modelConditioned.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(batch_dict['x_source'], \n",
    "                           batch_dict['x_source_length'], \n",
    "                           batch_dict['x_target'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # Update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
