{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing CNN In Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 7])\n",
      "tensor([[[-0.6152,  0.2864, -1.1016, -0.8544,  0.7845, -0.6594,  0.4768],\n",
      "         [-1.4123, -0.2553, -0.0138, -0.1109,  0.0127,  0.9263,  1.7683],\n",
      "         [-0.3195,  0.0191,  1.1332,  0.6086, -2.4340,  0.3850,  0.6449],\n",
      "         [ 0.9858, -1.3809, -0.4223,  0.7069,  0.5861,  0.1291, -0.2257],\n",
      "         [ 1.4455, -0.6440, -0.2073,  0.1421, -1.0787,  2.3157,  0.8062],\n",
      "         [ 0.6942,  0.1595,  0.9772, -1.9867,  1.5214,  0.3742, -0.2273],\n",
      "         [ 1.5826,  0.1628,  1.3762, -1.1712,  0.8213,  0.8788,  1.0851],\n",
      "         [ 2.0753,  0.0300,  1.1697, -0.4630,  0.3116, -1.6441,  0.2856],\n",
      "         [ 1.4191, -0.7567,  1.1687,  0.1607, -0.0891,  1.4693,  2.2877],\n",
      "         [ 0.0958, -0.2486, -0.3046,  0.9475,  0.4833,  0.1425, -2.4979]],\n",
      "\n",
      "        [[-0.4362,  0.2476,  1.5053, -1.0689,  0.4496,  0.4616, -1.3539],\n",
      "         [ 0.3971, -0.9727, -1.2485, -1.2355, -0.2908, -1.0833, -0.8490],\n",
      "         [ 0.7678,  0.0232,  1.5907, -0.7328, -1.7800,  0.9238,  0.7948],\n",
      "         [ 0.0715,  0.6918,  0.9277, -1.1739, -0.5631, -0.0126,  0.0071],\n",
      "         [ 0.5198,  2.4135, -1.5296,  0.3512,  0.8070,  0.7111,  1.6326],\n",
      "         [-0.9695, -0.5748,  0.4840, -0.1347, -1.1520,  0.1201,  0.7320],\n",
      "         [ 0.8041, -0.2374, -0.6096, -1.9431, -1.6306, -0.1214,  0.2403],\n",
      "         [-0.0748, -1.9016, -0.5778,  0.5745, -0.8839,  1.3720,  1.1696],\n",
      "         [ 1.6481, -0.4903, -1.7688, -0.0701, -0.4014, -0.1311, -0.7225],\n",
      "         [-1.0143,  1.6295,  0.0905,  0.3185, -0.8911,  0.8248, -0.2543]],\n",
      "\n",
      "        [[ 0.7849, -0.6585,  1.0565,  1.3358,  0.4066, -0.8010,  1.2613],\n",
      "         [-0.8019,  1.2752, -0.0287,  0.3842, -2.0881,  1.0153, -1.8837],\n",
      "         [ 0.7012,  0.6235,  0.9774, -0.4817,  0.4666,  0.1323,  0.3780],\n",
      "         [ 0.2341,  0.3598,  0.4994, -0.8016, -0.7966, -0.2458, -1.9104],\n",
      "         [-1.1241, -0.0744,  0.1096, -0.4996,  0.9940, -0.7083, -1.1623],\n",
      "         [ 0.5737, -0.4321,  0.7249, -1.4637, -1.4337,  0.4863,  1.3025],\n",
      "         [ 0.6531,  0.0130, -1.2802, -0.2435,  0.4175, -1.9066,  0.4407],\n",
      "         [ 1.6436,  1.6201,  0.2226,  0.0067,  1.3095, -0.5033,  0.0452],\n",
      "         [-0.9094, -0.4682,  0.2723,  2.3159, -2.1775,  0.6020,  0.6234],\n",
      "         [-0.3911, -0.7571, -0.4631,  0.0514,  0.8989,  0.4331, -1.4712]]])\n",
      "torch.Size([3, 16, 5])\n",
      "tensor([[[-1.5008e+00,  3.9893e-02, -4.4313e-01,  6.0782e-01,  1.0061e-01],\n",
      "         [ 2.5527e-01,  1.4496e-01, -3.5867e-01,  6.0760e-01,  6.7994e-01],\n",
      "         [ 1.4030e+00,  1.0010e-01,  6.5585e-01, -7.7306e-01,  1.0087e+00],\n",
      "         [-6.4540e-01, -7.1263e-01, -8.8172e-02,  2.5926e-01, -1.2638e+00],\n",
      "         [ 7.9961e-01, -2.2293e-01,  1.2981e+00, -1.4923e-02,  1.0243e+00],\n",
      "         [ 2.7172e-01, -4.2193e-02,  1.2796e-01, -3.8148e-01, -2.2516e-01],\n",
      "         [-1.0399e-02, -2.3641e-01,  8.0767e-01,  1.9375e-01, -4.5792e-01],\n",
      "         [ 4.4491e-01, -4.1781e-01,  9.5104e-01, -2.2380e-01, -1.0776e+00],\n",
      "         [-5.3386e-01,  1.0651e-01, -8.1992e-01,  1.0102e+00, -1.2010e+00],\n",
      "         [-4.9686e-02, -4.0702e-01, -1.4090e-01, -4.4366e-01, -4.2295e-01],\n",
      "         [-2.0024e-01,  4.7095e-01,  2.1504e-01,  1.4858e-02, -3.5445e-01],\n",
      "         [-1.0363e+00,  5.5357e-01, -9.1469e-01,  5.6206e-01,  3.7373e-01],\n",
      "         [-5.6215e-02,  1.2547e-01,  6.0821e-02, -2.3885e-01, -3.3882e-01],\n",
      "         [-6.4092e-01,  1.1242e-01,  2.6855e-01,  4.1517e-01, -4.1182e-01],\n",
      "         [ 8.4135e-01,  7.9305e-01,  2.6947e-01, -3.2789e-01,  6.1413e-01],\n",
      "         [-1.2176e+00,  4.6022e-01, -2.6152e-01, -3.0729e-01,  3.9317e-01]],\n",
      "\n",
      "        [[-4.1662e-01, -2.9333e-01,  9.8051e-01, -2.4383e-01,  9.0787e-01],\n",
      "         [ 1.0132e+00, -2.9215e-01, -7.6926e-02, -2.2161e-01, -2.4203e-01],\n",
      "         [-1.0273e+00,  3.7045e-01, -2.1286e-01, -1.1808e-01, -3.4213e-01],\n",
      "         [-1.7443e-01,  4.9963e-01,  1.0225e-01,  2.7450e-01, -2.0388e-02],\n",
      "         [-5.1700e-01, -4.6627e-01,  1.7908e-01, -7.9004e-01, -4.1517e-01],\n",
      "         [ 1.6626e-01, -4.3749e-01,  3.7148e-02, -7.3817e-01, -1.1741e-01],\n",
      "         [-1.2892e-02,  1.8851e-02,  8.6424e-02,  5.0431e-01, -2.5590e-01],\n",
      "         [-2.8921e-01, -7.1070e-01, -2.4575e-01,  5.6688e-01,  6.3044e-02],\n",
      "         [-4.7884e-01,  8.9408e-01,  5.3886e-01,  1.5438e-01,  7.4204e-01],\n",
      "         [ 1.1028e+00,  6.9525e-01, -4.1152e-01, -3.2289e-01,  3.7580e-01],\n",
      "         [ 4.1863e-01,  9.6193e-01,  7.1088e-01, -4.5537e-01,  6.9830e-01],\n",
      "         [-9.3901e-01,  1.5436e-01,  1.4647e+00,  9.2191e-01,  2.8471e-01],\n",
      "         [ 1.0531e-01,  8.0929e-01,  3.8395e-01,  4.6718e-01,  4.3956e-01],\n",
      "         [ 1.3257e+00, -4.2223e-02,  1.0668e-01, -6.3212e-01, -1.2873e-01],\n",
      "         [-1.1746e-01, -9.7792e-01, -7.6152e-01, -1.0279e-01,  3.7725e-01],\n",
      "         [ 1.5100e-01, -5.4536e-02,  1.1980e+00,  8.1859e-01,  8.8872e-01]],\n",
      "\n",
      "        [[-2.9141e-01, -5.4705e-01,  3.1584e-01,  9.6966e-01, -1.0488e+00],\n",
      "         [-5.8316e-02,  4.9051e-01, -8.2895e-01, -5.2882e-02,  5.6414e-01],\n",
      "         [ 6.4106e-01, -7.6724e-02, -6.1599e-01, -8.0996e-01,  9.2377e-01],\n",
      "         [-9.1391e-01, -1.3841e-01, -2.6913e-02,  3.3206e-01, -3.7600e-01],\n",
      "         [ 1.0766e-01, -4.1457e-02, -2.7259e-01,  3.4160e-01, -2.4229e-01],\n",
      "         [-8.1844e-02, -6.2299e-01, -8.5041e-01, -3.2682e-02, -4.0981e-02],\n",
      "         [-5.2078e-01,  5.4520e-01,  1.2982e-01,  2.1535e-01, -1.3414e+00],\n",
      "         [ 1.1490e-01,  2.5325e-04,  1.2418e+00, -6.3221e-01, -7.1712e-02],\n",
      "         [ 2.2239e-01,  3.5029e-01, -4.0047e-02,  2.4221e-01, -5.8243e-02],\n",
      "         [-3.9525e-01, -1.5283e-01, -7.8629e-01,  1.4047e+00,  5.8374e-01],\n",
      "         [ 1.6366e-01,  4.4038e-01, -6.7373e-01,  1.8624e+00, -4.0008e-01],\n",
      "         [ 3.8175e-01,  1.3496e-01,  2.9815e-01,  1.3660e-01,  1.3987e-01],\n",
      "         [-4.5191e-02,  2.8347e-02,  1.2070e+00, -4.0744e-03,  2.8152e-02],\n",
      "         [-1.4835e-01, -5.9205e-02,  2.8481e-01,  7.5578e-02,  1.8156e-01],\n",
      "         [ 1.4774e+00, -1.7003e-01,  5.0517e-01,  6.4477e-01, -1.0255e-01],\n",
      "         [ 8.7728e-02,  1.7076e-01,  6.8744e-01, -5.0401e-01,  5.3446e-01]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 3\n",
    "one_hot_size = 10\n",
    "sequence_width = 7\n",
    "\n",
    "data = torch.randn(batch_size, one_hot_size, sequence_width)\n",
    "conv1 = torch.nn.Conv1d(in_channels= one_hot_size,\n",
    "                       out_channels= 16,\n",
    "                       kernel_size= 3)\n",
    "\n",
    "intermediate1 = conv1(data)\n",
    "\n",
    "print(data.size())\n",
    "print(data)\n",
    "print(intermediate1.size())\n",
    "print(intermediate1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 3])\n",
      "tensor([[[ 3.4884e-01, -1.1175e-02, -1.3128e-01],\n",
      "         [-1.9049e-01, -6.8913e-03, -1.5062e-02],\n",
      "         [-2.0335e-02, -1.5935e-01,  4.1573e-01],\n",
      "         [-4.6385e-01, -3.5354e-02, -9.2693e-02],\n",
      "         [-9.1045e-01,  7.2159e-01, -4.0178e-01],\n",
      "         [-1.0951e-01, -3.2347e-01, -3.4278e-01],\n",
      "         [ 5.4106e-01,  5.1844e-01,  4.1303e-01],\n",
      "         [ 1.5835e-01, -1.0814e-01, -2.5527e-01],\n",
      "         [-1.9036e-01,  1.1965e-01, -4.3349e-02],\n",
      "         [ 6.6291e-02, -3.4554e-01,  1.7448e-02],\n",
      "         [ 9.3084e-02,  8.6087e-02,  5.1484e-01],\n",
      "         [ 1.8293e-01, -2.7182e-01,  5.7877e-01],\n",
      "         [ 5.3491e-01, -4.7961e-01,  6.8677e-01],\n",
      "         [-1.1597e-01,  1.3256e-01, -2.9553e-01],\n",
      "         [ 5.7226e-02, -3.2718e-01,  5.3132e-02],\n",
      "         [ 9.6140e-02, -1.4833e-01,  2.2031e-01],\n",
      "         [ 9.0165e-02, -2.9594e-01,  5.0259e-01],\n",
      "         [-5.0057e-01,  4.4018e-01, -2.3170e-01],\n",
      "         [-2.4062e-01,  1.4620e-01, -4.5579e-01],\n",
      "         [-4.4965e-01,  1.6483e-04, -1.9614e-01],\n",
      "         [ 2.7696e-01, -8.6658e-02,  1.4475e-01],\n",
      "         [-7.4681e-01,  4.4912e-01, -5.7348e-01],\n",
      "         [ 3.3834e-01, -1.3837e-01,  5.8132e-02],\n",
      "         [-4.0595e-01, -5.5124e-01,  7.0740e-01],\n",
      "         [-1.1227e-01,  1.6067e-01, -3.2985e-01],\n",
      "         [ 6.6014e-01, -8.2636e-02,  3.6528e-01],\n",
      "         [ 6.9489e-01,  1.0612e-01,  5.5564e-01],\n",
      "         [-4.4855e-01,  2.0205e-01,  3.0323e-01],\n",
      "         [-2.7497e-01,  8.6215e-02, -1.7431e-01],\n",
      "         [ 2.4936e-01, -5.2379e-02,  1.7991e-01],\n",
      "         [ 5.0715e-01,  1.4882e-01, -3.6425e-01],\n",
      "         [-5.5627e-03,  1.0358e-02,  3.0247e-01]],\n",
      "\n",
      "        [[ 2.3174e-02,  4.6133e-01,  6.1884e-02],\n",
      "         [ 6.7103e-02,  3.7137e-01, -1.4166e-01],\n",
      "         [ 3.4327e-01,  3.5430e-01,  4.3677e-01],\n",
      "         [ 1.3279e-01,  5.8954e-01,  3.3670e-01],\n",
      "         [-1.5826e-02, -2.9266e-01,  4.4298e-01],\n",
      "         [ 3.0841e-01, -3.0599e-04, -2.7809e-01],\n",
      "         [-4.2175e-01, -1.4258e-01, -3.6997e-02],\n",
      "         [ 9.8253e-02,  1.1138e-01,  3.5113e-01],\n",
      "         [ 2.3315e-02, -2.2231e-02,  9.0589e-02],\n",
      "         [-9.1628e-02,  8.4636e-01, -4.2573e-02],\n",
      "         [ 2.9877e-01,  2.0284e-01,  3.6559e-01],\n",
      "         [ 8.5448e-01,  3.2150e-01,  7.6544e-02],\n",
      "         [-9.8091e-01,  1.4719e-01, -4.0849e-02],\n",
      "         [-7.3218e-02,  8.1942e-02,  8.1490e-02],\n",
      "         [ 9.5127e-02, -2.6729e-01,  5.4540e-02],\n",
      "         [-3.7047e-01, -1.3130e-01, -3.5672e-01],\n",
      "         [-4.0122e-01, -3.1922e-03,  3.3344e-01],\n",
      "         [ 7.8067e-01,  9.7359e-01,  4.8112e-01],\n",
      "         [-4.2063e-01, -1.0361e-01, -1.5793e-02],\n",
      "         [ 1.2666e-01,  2.0435e-01,  7.2382e-02],\n",
      "         [-2.7689e-01,  1.5450e-01,  3.0453e-01],\n",
      "         [-2.4013e-01,  1.9441e-01,  3.5967e-01],\n",
      "         [ 1.2539e-01, -2.3783e-01, -2.6524e-01],\n",
      "         [ 3.1358e-01, -5.6543e-02, -1.3874e-01],\n",
      "         [-4.2058e-01,  1.9850e-01,  2.3249e-01],\n",
      "         [-2.0153e-01, -6.7373e-01, -3.7379e-01],\n",
      "         [ 1.7607e-01,  3.4138e-02, -3.6064e-01],\n",
      "         [-6.8017e-02, -3.2548e-01, -1.8700e-01],\n",
      "         [-6.8379e-01, -7.8702e-01,  3.4525e-02],\n",
      "         [ 1.2665e-01,  1.3144e-01, -3.8701e-01],\n",
      "         [-1.8328e-01,  2.8198e-01, -2.6150e-01],\n",
      "         [-3.9854e-01,  2.0516e-01,  1.6998e-01]],\n",
      "\n",
      "        [[ 7.8356e-01, -4.7991e-03,  2.0758e-02],\n",
      "         [-3.8954e-01, -1.6125e-01, -3.8375e-01],\n",
      "         [-1.5389e-01,  6.8213e-01, -3.5063e-01],\n",
      "         [-1.5667e-01, -6.1200e-02, -2.8128e-02],\n",
      "         [ 6.3843e-02, -2.5983e-03,  2.1675e-01],\n",
      "         [-3.7663e-01, -1.4792e-01,  5.2036e-01],\n",
      "         [ 2.6280e-01,  2.2043e-01, -2.9941e-01],\n",
      "         [ 1.9006e-01,  4.1542e-01,  3.0658e-01],\n",
      "         [-6.4299e-01,  7.1886e-01, -8.8416e-01],\n",
      "         [-3.6854e-02, -2.7453e-01, -4.3465e-01],\n",
      "         [ 3.5579e-01, -2.8017e-02,  3.4224e-01],\n",
      "         [ 2.8882e-01,  1.2044e-01,  1.6608e-02],\n",
      "         [ 1.1266e-01, -2.7474e-01,  2.3526e-02],\n",
      "         [ 2.3709e-01,  2.0426e-01, -3.5679e-01],\n",
      "         [-4.6501e-01,  2.5046e-01,  2.7723e-01],\n",
      "         [ 3.0664e-02, -1.0420e-01,  3.7439e-01],\n",
      "         [ 1.5726e-01, -2.5003e-02,  1.2513e-02],\n",
      "         [ 6.6576e-02,  7.5764e-01, -1.2644e-01],\n",
      "         [-7.9033e-02,  1.9927e-01, -2.0747e-01],\n",
      "         [-1.9989e-01, -2.0485e-01, -2.8281e-01],\n",
      "         [-4.4845e-02,  3.4843e-01,  8.1152e-02],\n",
      "         [ 3.2072e-01,  2.4793e-01, -8.3197e-02],\n",
      "         [-3.6546e-01, -2.1102e-02, -1.6244e-01],\n",
      "         [-7.8678e-01,  1.4185e-01,  2.5782e-01],\n",
      "         [ 5.7303e-02,  4.1821e-01,  4.4920e-01],\n",
      "         [ 1.0027e-01, -3.9113e-01,  7.0155e-01],\n",
      "         [ 1.7643e-01,  9.6620e-02, -7.2434e-01],\n",
      "         [-9.5085e-01,  1.4561e-02,  3.2213e-01],\n",
      "         [-4.6322e-01,  1.9500e-01,  1.0325e-01],\n",
      "         [ 1.0207e-01, -3.5757e-01,  1.0778e-01],\n",
      "         [ 3.3225e-01,  1.8577e-01, -8.4552e-01],\n",
      "         [ 2.5585e-01, -6.3781e-02, -5.8950e-02]]], grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([3, 64, 1])\n",
      "tensor([[[ 0.1601],\n",
      "         [ 0.3466],\n",
      "         [ 0.2518],\n",
      "         [ 0.0027],\n",
      "         [-0.3286],\n",
      "         [ 0.0253],\n",
      "         [-0.1809],\n",
      "         [-0.2393],\n",
      "         [ 0.3503],\n",
      "         [ 0.2343],\n",
      "         [-0.0786],\n",
      "         [ 0.4211],\n",
      "         [-0.0096],\n",
      "         [ 0.1727],\n",
      "         [-0.2736],\n",
      "         [ 0.0239],\n",
      "         [ 0.0082],\n",
      "         [ 0.1352],\n",
      "         [-0.1614],\n",
      "         [ 0.0235],\n",
      "         [-0.0404],\n",
      "         [-0.1172],\n",
      "         [ 0.2765],\n",
      "         [ 0.1450],\n",
      "         [ 0.3699],\n",
      "         [-0.1721],\n",
      "         [ 0.0027],\n",
      "         [ 0.0547],\n",
      "         [ 0.0324],\n",
      "         [-0.0658],\n",
      "         [ 0.0308],\n",
      "         [ 0.2768],\n",
      "         [ 0.1116],\n",
      "         [ 0.2562],\n",
      "         [ 0.0434],\n",
      "         [ 0.2965],\n",
      "         [-0.3295],\n",
      "         [-0.1709],\n",
      "         [ 0.1154],\n",
      "         [ 0.3841],\n",
      "         [ 0.0392],\n",
      "         [ 0.0240],\n",
      "         [-0.2902],\n",
      "         [-0.2156],\n",
      "         [ 0.0159],\n",
      "         [ 0.3569],\n",
      "         [ 0.2473],\n",
      "         [ 0.0418],\n",
      "         [ 0.1028],\n",
      "         [-0.1238],\n",
      "         [ 0.1503],\n",
      "         [-0.2566],\n",
      "         [ 0.1683],\n",
      "         [-0.4356],\n",
      "         [ 0.2074],\n",
      "         [ 0.4684],\n",
      "         [-0.3831],\n",
      "         [ 0.5003],\n",
      "         [-0.0400],\n",
      "         [-0.0928],\n",
      "         [-0.2473],\n",
      "         [-0.4405],\n",
      "         [-0.2377],\n",
      "         [ 0.2027]],\n",
      "\n",
      "        [[-0.2071],\n",
      "         [-0.0379],\n",
      "         [-0.1640],\n",
      "         [ 0.2246],\n",
      "         [ 0.2557],\n",
      "         [ 0.2136],\n",
      "         [ 0.0925],\n",
      "         [-0.0047],\n",
      "         [-0.1307],\n",
      "         [-0.2176],\n",
      "         [ 0.2150],\n",
      "         [-0.1452],\n",
      "         [-0.5080],\n",
      "         [-0.2363],\n",
      "         [ 0.2126],\n",
      "         [-0.4041],\n",
      "         [ 0.0045],\n",
      "         [ 0.1367],\n",
      "         [ 0.0202],\n",
      "         [-0.0479],\n",
      "         [ 0.1512],\n",
      "         [ 0.0442],\n",
      "         [-0.1584],\n",
      "         [-0.0361],\n",
      "         [ 0.0586],\n",
      "         [ 0.1013],\n",
      "         [-0.2784],\n",
      "         [-0.3447],\n",
      "         [-0.0215],\n",
      "         [ 0.4158],\n",
      "         [ 0.1119],\n",
      "         [ 0.2617],\n",
      "         [-0.0266],\n",
      "         [ 0.0047],\n",
      "         [ 0.2925],\n",
      "         [-0.0729],\n",
      "         [ 0.2088],\n",
      "         [ 0.1118],\n",
      "         [-0.0666],\n",
      "         [-0.0958],\n",
      "         [-0.1093],\n",
      "         [ 0.0356],\n",
      "         [ 0.1907],\n",
      "         [-0.0253],\n",
      "         [-0.1205],\n",
      "         [ 0.2068],\n",
      "         [ 0.1479],\n",
      "         [-0.1352],\n",
      "         [ 0.3549],\n",
      "         [ 0.0545],\n",
      "         [ 0.1327],\n",
      "         [ 0.4766],\n",
      "         [-0.2720],\n",
      "         [ 0.1305],\n",
      "         [-0.0175],\n",
      "         [-0.0846],\n",
      "         [ 0.2473],\n",
      "         [-0.0484],\n",
      "         [ 0.1626],\n",
      "         [ 0.0327],\n",
      "         [ 0.3111],\n",
      "         [-0.1505],\n",
      "         [ 0.1508],\n",
      "         [ 0.2713]],\n",
      "\n",
      "        [[-0.2143],\n",
      "         [-0.0579],\n",
      "         [-0.0313],\n",
      "         [-0.1162],\n",
      "         [-0.1548],\n",
      "         [-0.2320],\n",
      "         [-0.2008],\n",
      "         [ 0.0564],\n",
      "         [-0.1214],\n",
      "         [ 0.0379],\n",
      "         [-0.2826],\n",
      "         [ 0.1581],\n",
      "         [-0.3957],\n",
      "         [ 0.0113],\n",
      "         [-0.0058],\n",
      "         [-0.3450],\n",
      "         [ 0.2110],\n",
      "         [-0.0649],\n",
      "         [-0.0555],\n",
      "         [-0.1926],\n",
      "         [-0.1899],\n",
      "         [-0.2707],\n",
      "         [-0.0011],\n",
      "         [-0.0469],\n",
      "         [ 0.1969],\n",
      "         [ 0.1829],\n",
      "         [-0.2596],\n",
      "         [-0.0092],\n",
      "         [-0.4012],\n",
      "         [ 0.0676],\n",
      "         [-0.1614],\n",
      "         [ 0.0679],\n",
      "         [-0.0359],\n",
      "         [ 0.1693],\n",
      "         [-0.1749],\n",
      "         [-0.1749],\n",
      "         [-0.4732],\n",
      "         [ 0.0632],\n",
      "         [-0.0643],\n",
      "         [ 0.2280],\n",
      "         [-0.1582],\n",
      "         [ 0.1605],\n",
      "         [-0.2006],\n",
      "         [ 0.1942],\n",
      "         [-0.0550],\n",
      "         [ 0.3211],\n",
      "         [ 0.1699],\n",
      "         [-0.1999],\n",
      "         [ 0.1353],\n",
      "         [-0.0797],\n",
      "         [-0.0273],\n",
      "         [ 0.4070],\n",
      "         [ 0.1013],\n",
      "         [ 0.0630],\n",
      "         [ 0.1678],\n",
      "         [ 0.1208],\n",
      "         [ 0.0406],\n",
      "         [ 0.1624],\n",
      "         [-0.1264],\n",
      "         [ 0.2451],\n",
      "         [ 0.1628],\n",
      "         [-0.0385],\n",
      "         [ 0.3215],\n",
      "         [-0.0052]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "conv2 = torch.nn.Conv1d(in_channels=16,\n",
    "                       out_channels=32,\n",
    "                       kernel_size=3)\n",
    "conv3 = torch.nn.Conv1d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3)\n",
    "\n",
    "intermediate2 = conv2(intermediate1)\n",
    "intermediate3 = conv3(intermediate2)\n",
    "\n",
    "print(intermediate2.size())\n",
    "print(intermediate2)\n",
    "\n",
    "print(intermediate3.size())\n",
    "print(intermediate3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surname Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary class\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to extract and process vocabularies for mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {\n",
    "            idx: token for token, idx in self._token_to_idx.items()\n",
    "        }\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializeable(self):\n",
    "        \"\"\"return a serializeable dictionary\"\"\"\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token': self._unk_token\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializeable(cls, contents):\n",
    "        \"\"\"create vocabulary object from serialize dictionary\"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add a token and return it's index\"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"get the index of a token \n",
    "        if not exist returns the unk_index\"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, character_vocab, nationality_vocab, max_surname_length):\n",
    "        self.character_vocab = character_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self.max_surname_length = max_surname_length\n",
    "        \n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"Create one_hot vector for review\n",
    "        Args:\n",
    "            surname (str): the surname\n",
    "        Returns:\n",
    "            one_hot_matrix (ndarray): matrix of one hot vectors\n",
    "        \"\"\"\n",
    "        # X = surname_vocab\n",
    "        # Y = max_surname_length\n",
    "        one_hot_matrix_size = (len(self.character_vocab), self.max_surname_length)\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "        \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.character_vocab.lookup_token(character)\n",
    "            one_hot_matrix[character_index][position_index] = 1\n",
    "            \n",
    "        return one_hot_matrix\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, dataframe, cutoff=25):\n",
    "        \"\"\"Instantiate a ReviewVector from dataset\"\"\"\n",
    "        \n",
    "        character_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "        max_surname_length = 0\n",
    "        \n",
    "        for index, row in dataframe.iterrows():\n",
    "            # check for longgest surname length\n",
    "            max_surname_length = max(max_surname_length, len(row.surname))\n",
    "            \n",
    "            for letter in row.surname:\n",
    "                character_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "            \n",
    "        return cls(surname_vocab, nationality_vocab, max_surname_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.surname_df[self.surname_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.surname_df[self.surname_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights to use with cross entropy\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv, cuda=False):\n",
    "        \"\"\"Load dataset from csv and returns the dataset object\n",
    "        and vectorizer\"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split == 'train']\n",
    "        return cls(surname_df,\n",
    "                   SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Get vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        \"\"\"Set the split from data\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dict of the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        surname_matrix = self._vectorizer.vectorize(row.surname)\n",
    "        \n",
    "        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        \n",
    "        return {\n",
    "            'x_data' : surname_matrix,\n",
    "            'y_target' : nationality_index\n",
    "        }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given the batch size return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\"CNN Classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels, dropout):\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self._dropout = dropout\n",
    "        \n",
    "        # convolution layer\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=initial_num_channels,\n",
    "                     out_channels=num_channels,\n",
    "                     kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels,\n",
    "                     out_channels=num_channels,\n",
    "                     kernel_size=3,\n",
    "                     stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels,\n",
    "                     out_channels=num_channels,\n",
    "                     kernel_size=3,\n",
    "                     stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels,\n",
    "                     out_channels=num_channels,\n",
    "                     kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"Forward pass the network given the x_in\n",
    "        Args:\n",
    "            x_in (torch.Tensor): input data Tensor\n",
    "            with shape (batch_size, initial_num_channels, max_surname_length)\n",
    "        Returns:\n",
    "            resulting tensor with shape (batch, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        features = self.convnet(x_in).squeeze(dim=2)\n",
    "        prediction_vector = self.fc(features)\n",
    "    \n",
    "        # Adding dropout\n",
    "        # only applied in training\n",
    "        dropout = torch.nn.Dropout(p=self._dropout)\n",
    "        if self.training:\n",
    "            prediction_vector = self.fc2(dropout(intermediate_vector))\n",
    "        else:\n",
    "            prediction_vector = self.fc2(intermediate_vector)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            prediction_vector = torch.softmax(prediction_vector, dim=1)\n",
    "        \n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data information\n",
    "    frequency_cutoff = 25,\n",
    "    model_state_file = '/content/drive/My Drive/Colab Notebooks/Data/surname_dataset/model.pth',\n",
    "    surname_csv = '/content/drive/My Drive/Colab Notebooks/Data/surname_dataset/surnames_with_splits.csv',\n",
    "    save_dir = '/content/drive/My Drive/Colab Notebooks/Data/surname_dataset/',\n",
    "    vectorizer_file = '/content/drive/My Drive/Colab Notebooks/Data/surname_dataset/vectorizer.json',\n",
    "    # Model HyperParameters\n",
    "    hidden_dim = 500,\n",
    "    num_channels = 256,\n",
    "    # Training HyperParameters\n",
    "    batch_size = 128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    cuda=True,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available  cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /content/drive/My Drive/Colab Notebooks/Data/surname_dataset/surnames_with_splits.csv does not exist: '/content/drive/My Drive/Colab Notebooks/Data/surname_dataset/surnames_with_splits.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-669a882b285e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# dataset object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSurnameDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dataset_and_make_vectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurname_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6df648cb5d9d>\u001b[0m in \u001b[0;36mload_dataset_and_make_vectorizer\u001b[1;34m(cls, surname_csv, cuda)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"Load dataset from csv and returns the dataset object\n\u001b[0;32m     32\u001b[0m         and vectorizer\"\"\"\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msurname_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msurname_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mtrain_surname_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurname_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msurname_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         return cls(surname_df,\n",
      "\u001b[1;32mc:\\users\\miqda\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\miqda\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\miqda\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\miqda\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\miqda\\anaconda3\\envs\\datascience\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /content/drive/My Drive/Colab Notebooks/Data/surname_dataset/surnames_with_splits.csv does not exist: '/content/drive/My Drive/Colab Notebooks/Data/surname_dataset/surnames_with_splits.csv'"
     ]
    }
   ],
   "source": [
    "# create variables to record\n",
    "# the training process\n",
    "def make_train_state(args):\n",
    "    return {\n",
    "        'epoch_index':0,\n",
    "        'train_loss':[],\n",
    "        'train_acc':[],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': -1,\n",
    "        'test_acc': -1,\n",
    "    }\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "  args.cuda = True\n",
    "else:\n",
    "  args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Device available \", args.device)\n",
    "\n",
    "# dataset object\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "# vectorizer\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# classifier\n",
    "classifier = SurnameClassifier(initial_num_channels=len(vectorizer.surname_vocab),\n",
    "                               num_classes = len(vectorizer.nationality_vocab),\n",
    "                               num_channels=args.num_channels,\n",
    "                               dropout=args.dropout)\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "print(\"Input dim \", len(vectorizer.surname_vocab))\n",
    "print(\"Output dim \", len(vectorizer.nationality_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training loop\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size)-1, \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size)-1, \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        # setup batch generator\n",
    "        # set loss and train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset=dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "      \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "      \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1 zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "            # step 2 compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "          \n",
    "            # step 3 compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "          \n",
    "            # step 4 use loss to produce gradients\n",
    "            loss.backward()\n",
    "          \n",
    "            # step 5 use optimizer to take the gradient step\n",
    "            optimizer.step()\n",
    "          \n",
    "            # step 6 compute the acccuracy\n",
    "            acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "          \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "      \n",
    "        # Iterate over val dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "      \n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "      \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "          \n",
    "            # step 2. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "          \n",
    "            # step 3. compute the accuracy\n",
    "            acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "          \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
