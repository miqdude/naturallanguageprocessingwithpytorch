{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from argparse import Namespace\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(object):\n",
    "    \"\"\"Class to extract and process vocabularies for mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {\n",
    "            idx: token for token, idx in self._token_to_idx.items()\n",
    "        }\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "        \n",
    "        # add begin and end sequence token\n",
    "        self._begin_of_seq_token = \"<BEGIN-OF-SEQUENCE>\"\n",
    "        self._end_of_seq_token = \"<END-OF-SEQUENCE>\"\n",
    "        \n",
    "        self.begin_seq_index = self.add_token(self._begin_of_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_of_seq_token)\n",
    "\n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "            \n",
    "    def to_serializeable(self):\n",
    "        \"\"\"return a serializeable dictionary\"\"\"\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx,\n",
    "            'mask_token': self._mask_token,\n",
    "            'add_unk': self._add_unk,\n",
    "            'unk_token': self._unk_token\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializeable(cls, contents):\n",
    "        \"\"\"create vocabulary object from serialize dictionary\"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add a token and return it's index\"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"get the index of a token \n",
    "        if not exist returns the unk_index\"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary class\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to extract and process vocabularies for mapping\"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        \n",
    "        self._idx_to_token = {\n",
    "            idx: token for token, idx in self._token_to_idx.items()\n",
    "        }\n",
    "\n",
    "    def to_serializeable(self):\n",
    "        \"\"\"return a serializeable dictionary\"\"\"\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializeable(cls, contents):\n",
    "        \"\"\"create vocabulary object from serialize dictionary\"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add a token and return it's index\"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"get the index of a token \n",
    "        if not exist returns the unk_index\"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index %d is not in the vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "    \n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        indices = [self.surname_vocab.begin_seq_index]\n",
    "        indices.extend(self.surname_vocab.lookup_token(token)\n",
    "                    for token in title.split(\" \"))\n",
    "        indices.append(self.surname_vocab.end_seq_index)\n",
    "\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        # create vector representation\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.surname_vocab.mask_index\n",
    "\n",
    "        return out_vector, len(indices)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df, cutoff=25):\n",
    "        nationality_vocab = Vocabulary()\n",
    "        character_vocab = SequenceVocabulary()\n",
    "        \n",
    "        for index, row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                character_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        \n",
    "        return cls(character_vocab, nationality_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surnames Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.train_df = self.surname_df[self.surname_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.surname_df[self.surname_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.surname_df[self.surname_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights to use with cross entropy\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv, cuda=False):\n",
    "        \"\"\"Load dataset from csv and returns the dataset object\n",
    "        and vectorizer\"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split == 'train']\n",
    "        return cls(surname_df,\n",
    "                   SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"Get vectorizer\"\"\"\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        \"\"\"Set the split from data\"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dict of the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        surname_vector, vec_length = self._vectorizer.vectorize(row.surname,\n",
    "                                                               self._max_seq_length)\n",
    "        \n",
    "        nationality_index = self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        \n",
    "        return {\n",
    "            'x_data' : surname_vector,\n",
    "            'y_target' : nationality_index,\n",
    "            'x_length': vec_length\n",
    "        }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given the batch size return the number of batches in the dataset\"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElmanRNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def _initialize_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "            \n",
    "        hiddens = []\n",
    "        \n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initialize_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "            \n",
    "        hidden_t = initial_hidden\n",
    "        \n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "            \n",
    "        hiddens = torch.stack(hiddens)\n",
    "        \n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "            \n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surnames RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_gather(y_out, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    \n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "    \n",
    "    return torch.stack(out)\n",
    "\n",
    "class SurnameRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
    "                rnn_hidden_size, batch_first=True, padding_idx=0):\n",
    "        super(SurnameRNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                               embedding_dim = embedding_size,\n",
    "                               padding_idx= padding_idx)\n",
    "        \n",
    "        self.rnn = ElmanRNN(input_size= embedding_size,\n",
    "                              hidden_size= rnn_hidden_size,\n",
    "                              batch_first= batch_first)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features= rnn_hidden_size,\n",
    "                            out_features = rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features= rnn_hidden_size,\n",
    "                            out_features = num_classes)\n",
    "        \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        \n",
    "        x_embed = self.emb(x_in)\n",
    "        y_out = self.rnn(x_embed)\n",
    "        \n",
    "        dropout = torch.Dropout(p=.5)\n",
    "        \n",
    "        if x_lengths is not None:\n",
    "            y_out = column_gather(y_out, x_lengths)\n",
    "        else:\n",
    "            y_out = y_out[:, -1, :]\n",
    "            \n",
    "        y_out = dropout(y_out)\n",
    "        y_out = F.relu(self.fc1(y_out))\n",
    "        y_out = dropout(y_out)\n",
    "        y_out = self.fc2(y_out)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data information\n",
    "    frequency_cutoff = 25,\n",
    "    model_state_file = '/content/drive/My Drive/Colab Notebooks/Data/model.pth',\n",
    "    surnames_csv = '/content/drive/My Drive/Colab Notebooks/Data/surnames_with_splits.csv',\n",
    "    save_dir = '/content/drive/My Drive/Colab Notebooks/Data',\n",
    "    vectorizer_file = '/content/drive/My Drive/Colab Notebooks/Data/vectorizer.json',\n",
    "    # Model HyperParameters\n",
    "    char_embedding_size = 100,\n",
    "    rnn_hidden_size=64,\n",
    "    # Training HyperParameters\n",
    "    batch_size = 128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    momentum=0.1,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    cuda=True,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "    ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables to record\n",
    "# the training process\n",
    "def make_train_state(args):\n",
    "    return {\n",
    "        'epoch_index':0,\n",
    "        'train_loss':[],\n",
    "        'train_acc':[],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'test_loss': -1,\n",
    "        'test_acc': -1,\n",
    "    }\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "  args.cuda = True\n",
    "else:\n",
    "  args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Device available \", args.device)\n",
    "\n",
    "# dataset object\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "# vectorizer\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# classifier\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = SurnameClassifier(embedding_size=args.char_embedding_size, \n",
    "                               num_embeddings=len(vectorizer.char_vocab),\n",
    "                               num_classes=len(vectorizer.nationality_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index)\n",
    "classifier.to(args.device)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "\n",
    "print(\"Input dim \", len(vectorizer.surname_vocab))\n",
    "print(\"Output dim \", len(vectorizer.nationality_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training loop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size)-1, \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size)-1, \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "        # setup batch generator\n",
    "        # set loss and train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset=dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "      \n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "      \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1 zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "          \n",
    "            # step 2 compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "          \n",
    "            # step 3 compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "          \n",
    "            # step 4 use loss to produce gradients\n",
    "            loss.backward()\n",
    "          \n",
    "            # step 5 use optimizer to take the gradient step\n",
    "            optimizer.step()\n",
    "          \n",
    "            # step 6 compute the acccuracy\n",
    "            acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "          \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "      \n",
    "        # Iterate over val dataset\n",
    "        # setup: batch generator, set loss and acc to 0, set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                        batch_size=args.batch_size,\n",
    "                                        device=args.device)\n",
    "      \n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "      \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # step 1. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "          \n",
    "            # step 2. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_batch = loss.item()\n",
    "            running_loss += (loss_batch - running_loss) / (batch_index + 1)\n",
    "          \n",
    "            # step 3. compute the accuracy\n",
    "            acc_batch = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_batch - running_acc) / (batch_index + 1)\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "            \n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "          \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
